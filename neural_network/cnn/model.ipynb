{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1025, 1293) (800, 10, 1293) (800, 128, 1293) (800, 12, 1293) (800, 8)\n"
     ]
    }
   ],
   "source": [
    "# load the .npz file of features\n",
    "f = np.load(os.getcwd()+\"/MusicFeatures.npz\")\n",
    "S = f['spec']\n",
    "mfcc = f['mfcc']\n",
    "mel = f['mel']\n",
    "chroma = f['chroma']\n",
    "y = f['target']\n",
    "\n",
    "print(S.shape, mfcc.shape, mel.shape, chroma.shape, y.shape)\n",
    "# split train-test data\n",
    "S_train, S_test, mfcc_train, mfcc_test, mel_train, mel_test, chroma_train, chroma_test, y_train, y_test = train_test_split(S, mfcc, mel, chroma, y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram\n",
    "# maximum1 = np.amax(S_train)\n",
    "# S_train = S_train/np.amax(maximum1)\n",
    "# S_test = S_test/np.amax(maximum1)\n",
    "\n",
    "# S_train = S_train.astype(np.float32)\n",
    "# S_test = S_test.astype(np.float32)\n",
    "\n",
    "# N, row, col = S_train.shape\n",
    "# S_train = S_train.reshape((N, row, col, 1))\n",
    "\n",
    "# N, row, col = S_test.shape\n",
    "# S_test = S_test.reshape((N, row, col, 1))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Assuming S_train and S_test are your spectrogram datasets\n",
    "\n",
    "# Normalize the spectrograms in batches\n",
    "batch_size = 32\n",
    "\n",
    "def normalize_in_batches(data):\n",
    "    max_vals = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        max_val = np.amax(batch)\n",
    "        max_vals.append(max_val)\n",
    "        batch = batch / max_val\n",
    "        data[i:i+batch_size] = batch\n",
    "    return data, max_vals\n",
    "\n",
    "# Normalize training data\n",
    "S_train_normalized, max_vals_train = normalize_in_batches(S_train.copy())\n",
    "\n",
    "# Normalize test data using max values from training\n",
    "max_vals_train = np.array(max_vals_train)\n",
    "S_test_normalized, _ = normalize_in_batches(S_test.copy())\n",
    "\n",
    "# Apply the normalization factor to test data\n",
    "S_test_normalized /= max_vals_train.mean()\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "S_train_tensor = torch.from_numpy(S_train_normalized).unsqueeze(3).permute(0, 3, 1, 2).float()\n",
    "S_test_tensor = torch.from_numpy(S_test_normalized).unsqueeze(3).permute(0, 3, 1, 2).float()\n",
    "\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(S_train_tensor)\n",
    "test_dataset = TensorDataset(S_test_tensor)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCC\n",
    "newtrain_mfcc = np.empty((mfcc_train.shape[0], 120, 600))\n",
    "newtest_mfcc = np.empty((mfcc_test.shape[0], 120, 600))\n",
    "\n",
    "for i in range(mfcc_train.shape[0]) :\n",
    "    curr = mfcc_train[i]\n",
    "    curr = cv2.resize(curr, (600, 120))\n",
    "    newtrain_mfcc[i] = curr\n",
    "\n",
    "mfcc_train = newtrain_mfcc\n",
    "\n",
    "for i in range(mfcc_test.shape[0]) :\n",
    "\n",
    "  curr = mfcc_test[i]\n",
    "  curr = cv2.resize(curr, (600, 120))\n",
    "  newtest_mfcc[i] = curr\n",
    "\n",
    "mfcc_test = newtest_mfcc\n",
    "\n",
    "mfcc_train = mfcc_train.astype(np.float32)\n",
    "mfcc_test = mfcc_test.astype(np.float32)\n",
    "\n",
    "N, row, col = mfcc_train.shape\n",
    "mfcc_train = mfcc_train.reshape((N, row, col, 1))\n",
    "\n",
    "N, row, col = mfcc_test.shape\n",
    "mfcc_test = mfcc_test.reshape((N, row, col, 1))\n",
    "\n",
    "mean_data = np.mean(mfcc_train)\n",
    "std_data = np.std(mfcc_train)\n",
    "\n",
    "mfcc_train = (mfcc_train - mean_data)/ std_data\n",
    "mfcc_test = (mfcc_test - mean_data)/ std_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-Spectrogram\n",
    "\n",
    "maximum = np.amax(mel_train)\n",
    "mel_train = mel_train/np.amax(maximum)\n",
    "mel_test = mel_test/np.amax(maximum)\n",
    "\n",
    "mel_train = mel_train.astype(np.float32)\n",
    "mel_test = mel_test.astype(np.float32)\n",
    "\n",
    "N, row, col = mel_train.shape\n",
    "mel_train = mel_train.reshape((N, row, col, 1))\n",
    "\n",
    "N, row, col = mel_test.shape\n",
    "mel_test = mel_test.reshape((N, row, col, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Spectrogram train-test\n",
    "np.savez_compressed(os.getcwd()+\"/new_spectrogram_train_test.npz\", S_train= S_train, S_test= S_test, y_train = y_train, y_test= y_test)\n",
    "\n",
    "# Save MFCC train-test\n",
    "np.savez_compressed(os.getcwd()+\"/new_mfcc_train_test.npz\", mfcc_train= mfcc_train, mfcc_test= mfcc_test, y_train = y_train, y_test= y_test)\n",
    "\n",
    "# Save Mel-Spectrogram train-test\n",
    "np.savez_compressed(os.getcwd()+\"/new_mel_train_test.npz\", mel_train= mel_train, mel_test= mel_test, y_train = y_train, y_test= y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.16 GiB for an array with shape (848208000,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Iterate through the entire dataset in smaller chunks\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, dim1_length, batch_size):\n\u001b[1;32m---> 78\u001b[0m     S_train_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mspec_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[idx:idx \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     79\u001b[0m     y_train_chunk \u001b[38;5;241m=\u001b[39m spec_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m][idx:idx \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     81\u001b[0m     y_train_chunk \u001b[38;5;241m=\u001b[39m y_train_chunk\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\numpy\\lib\\npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\numpy\\lib\\format.py:814\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    801\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfromfile(fp, dtype\u001b[38;5;241m=\u001b[39mdtype, count\u001b[38;5;241m=\u001b[39mcount)\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;66;03m# If dtype.itemsize == 0 then there's nothing more to read\u001b[39;00m\n\u001b[0;32m    818\u001b[0m         max_read_count \u001b[38;5;241m=\u001b[39m BUFFER_SIZE \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmin\u001b[39m(BUFFER_SIZE, dtype\u001b[38;5;241m.\u001b[39mitemsize)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.16 GiB for an array with shape (848208000,) and data type float32"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Load Spectrogram Train-test data\n",
    "spec_file = np.load(os.getcwd() + \"/new_spectrogram_train_test.npz\")\n",
    "\n",
    "def create_data_loader(data, labels, batch_size):\n",
    "    tensor_data = torch.from_numpy(data).unsqueeze(1).float()\n",
    "    tensor_labels = torch.from_numpy(labels).long()\n",
    "    dataset = TensorDataset(tensor_data, tensor_labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# Model 1 for Spectrogram\n",
    "\n",
    "chunk_size = 1000  # Define the chunk size\n",
    "dim1_length = 800\n",
    "# Load and process the data in smaller chunks\n",
    "num_samples_train = dim1_length\n",
    "num_samples_test = dim1_length\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "# S_train_tensor = torch.from_numpy(S_train).unsqueeze(1).float()\n",
    "# S_test_tensor = torch.from_numpy(S_test).unsqueeze(1).float()\n",
    "# y_train_tensor = torch.from_numpy(y_train).long()\n",
    "# y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "\n",
    "# Define the CNN model in PyTorch\n",
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=4, stride=4, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv4(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv5(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SpectrogramCNN()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Assuming S_train and y_train are PyTorch tensors\n",
    "train_dataset = TensorDataset(S_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop in a streaming-like manner\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate through the entire dataset in smaller chunks\n",
    "    for idx in range(0, dim1_length, batch_size):\n",
    "        S_train_chunk = spec_file['S_train'][idx:idx + batch_size]\n",
    "        y_train_chunk = spec_file['y_train'][idx:idx + batch_size]\n",
    "\n",
    "        y_train_chunk = y_train_chunk.flatten()\n",
    "\n",
    "        train_loader = create_data_loader(S_train_chunk, y_train_chunk, batch_size)\n",
    "\n",
    "        # Training loop for each chunk\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch [{epoch + 1}, {i + 1}], Loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'models/new_spec_model_spectrogram1.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming S_train, y_train, S_test, y_test are PyTorch tensors\n",
    "\n",
    "# Load the saved PyTorch model\n",
    "model = SpectrogramCNN()\n",
    "model.load_state_dict(torch.load('new_spec_model_spectrogram1.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    _, predicted = torch.max(predictions, 1)\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    total = targets.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy * 100\n",
    "\n",
    "# Function to convert PyTorch tensor to numpy array\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "# Evaluate on training set\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(S_train)\n",
    "    train_accuracy = calculate_accuracy(train_outputs, torch.argmax(y_train, dim=1))\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate on testing set\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(S_test)\n",
    "    test_accuracy = calculate_accuracy(test_outputs, torch.argmax(y_test, dim=1))\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Create a confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    conf_mat = np.round(conf_mat, 2)\n",
    "\n",
    "    conf_mat_df = pd.DataFrame(conf_mat, columns=class_names, index=class_names)\n",
    "\n",
    "    plt.figure(figsize=(10, 7), dpi=200)\n",
    "    sn.set(font_scale=1.4)\n",
    "    sn.heatmap(conf_mat_df, annot=True, annot_kws={\"size\": 16})  # font size\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.getcwd() + \"/new_spec_conf_mat1.png\")\n",
    "\n",
    "# Convert predictions and labels to numpy arrays\n",
    "y_true_numpy = to_numpy(torch.argmax(y_test, dim=1))\n",
    "y_pred_numpy = to_numpy(torch.argmax(test_outputs, dim=1))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "class_names = ['Experimental', 'Rock', 'Instrumental', 'International', 'Hip-Hop', 'Folk', 'Electronic', 'Pop']\n",
    "plot_confusion_matrix(y_true_numpy, y_pred_numpy, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming you've loaded your data into S_train, S_test, y_train, y_test\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "S_train_tensor = torch.tensor(S_train, dtype=torch.float32)\n",
    "S_test_tensor = torch.tensor(S_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define the model in PyTorch\n",
    "class SpectrogramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectrogramModel, self).__init__()\n",
    "        # Define layers similar to Keras model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement forward pass\n",
    "        return x\n",
    "\n",
    "model = SpectrogramModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(S_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(S_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Testing Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix and visualization\n",
    "# Please note that the PyTorch confusion matrix and visualization require additional steps beyond this outline.\n",
    "# You can utilize sklearn's confusion_matrix and seaborn's heatmap similarly to the Keras code for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
